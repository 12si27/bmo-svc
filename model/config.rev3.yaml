K_step: 1000
accumulate_grad_batches: 1
audio_num_mel_bins: 128
audio_sample_rate: 44100
binarization_args:
  shuffle: false
  with_align: true
  with_f0: true
  with_hubert: true
  with_spk_embed: false
  with_wav: false
binarizer_cls: preprocessing.SVCpre.SVCBinarizer
binary_data_dir: data/binary/BMO
check_val_every_n_epoch: 10
choose_test_manually: false
clip_grad_norm: 1
config_path: training/config_nsf.yaml
content_cond_steps: []
credits: This model has been trained using liee (feminine). Please remember to credit
  the author of the model during usage.
cwt_add_f0_loss: false
cwt_hidden_size: 128
cwt_layers: 2
cwt_loss: l1
cwt_std_scale: 0.8
datasets:
- opencpop
debug: false
dec_ffn_kernel_size: 9
dec_layers: 4
decay_steps: 40000
decoder_type: fft
dict_dir: ''
diff_decoder_type: wavenet
diff_loss_type: l2
dilation_cycle_length: 4
dropout: 0.1
ds_workers: 4
dur_enc_hidden_stride_kernel:
- 0,2,3
- 0,2,3
- 0,1,3
dur_loss: mse
dur_predictor_kernel: 3
dur_predictor_layers: 5
enc_ffn_kernel_size: 9
enc_layers: 4
encoder_K: 8
encoder_type: fft
endless_ds: true
f0_bin: 256
f0_max: 1100.0
f0_min: 40.0
ffn_act: gelu
ffn_padding: SAME
fft_size: 2048
fmax: 16000
fmin: 40
fs2_ckpt: ''
gaussian_start: true
gen_dir_name: ''
gen_tgt_spk_id: -1
hidden_size: 256
hop_size: 512
hubert_gpu: true
hubert_path: checkpoints/hubert/hubert_soft.pt
infer: false
keep_bins: 128
lambda_commit: 0.25
lambda_energy: 0.0
lambda_f0: 1.0
lambda_ph_dur: 0.3
lambda_sent_dur: 1.0
lambda_uv: 1.0
lambda_word_dur: 1.0
load_ckpt: pretrain/model_ckpt_steps_600000.ckpt
log_interval: 100
loud_norm: false
lr: 0.0008
max_beta: 0.02
max_epochs: 3000
max_eval_sentences: 1
max_eval_tokens: 60000
max_frames: 42000
max_input_tokens: 60000
max_sentences: 12
max_tokens: 128000
max_updates: 1000000
mel_loss: ssim:0.5|l1:0.5
mel_vmax: 1.5
mel_vmin: -6.0
min_level_db: -120
no_fs2: true
norm_type: gn
num_ckpt_keep: 10
num_heads: 2
num_sanity_val_steps: 1
num_spk: 1
num_test_samples: 0
num_valid_plots: 10
optimizer_adam_beta1: 0.9
optimizer_adam_beta2: 0.98
out_wav_norm: false
pe_ckpt: checkpoints/0102_xiaoma_pe/model_ckpt_steps_60000.ckpt
pe_enable: false
perform_enhance: true
pitch_ar: false
pitch_enc_hidden_stride_kernel:
- 0,2,5
- 0,2,5
- 0,2,5
pitch_extractor: parselmouth
pitch_loss: l2
pitch_norm: log
pitch_type: frame
pndm_speedup: 10
pre_align_args:
  allow_no_txt: false
  denoise: false
  forced_align: mfa
  txt_processor: zh_g2pM
  use_sox: true
  use_tone: false
pre_align_cls: data_gen.singing.pre_align.SingingPreAlign
predictor_dropout: 0.5
predictor_grad: 0.1
predictor_hidden: -1
predictor_kernel: 5
predictor_layers: 5
prenet_dropout: 0.5
prenet_hidden_size: 256
pretrain_fs_ckpt: ''
processed_data_dir: xxx
profile_infer: false
raw_data_dir: data/raw/BMO
ref_norm_layer: bn
rel_pos: true
reset_phone_dict: true
residual_channels: 384
residual_layers: 20
save_best: false
save_ckpt: true
save_codes:
- configs
- modules
- src
- utils
save_f0: true
save_gt: false
schedule_type: linear
seed: 1234
sort_by_len: true
speaker_id: BMO
spec_max:
- 0.3086455762386322
- -0.05081344023346901
- -0.091364286839962
- 0.17670805752277374
- 0.32919377088546753
- 0.7401334643363953
- 0.8476316928863525
- 0.9772645831108093
- 0.9442583918571472
- 1.1599498987197876
- 1.106584072113037
- 1.0887508392333984
- 1.1090424060821533
- 1.0557878017425537
- 1.064404010772705
- 1.1336365938186646
- 1.124609351158142
- 1.0842162370681763
- 1.0729221105575562
- 1.0794498920440674
- 1.1317384243011475
- 1.1482346057891846
- 1.0506930351257324
- 1.0020092725753784
- 1.0433634519577026
- 1.1376157999038696
- 1.1245145797729492
- 1.1065272092819214
- 1.1475305557250977
- 1.1186251640319824
- 1.0792533159255981
- 1.118129849433899
- 1.1005396842956543
- 1.0732513666152954
- 1.1956744194030762
- 1.1639430522918701
- 1.0893000364303589
- 1.1351505517959595
- 1.0529818534851074
- 0.986912190914154
- 1.0486993789672852
- 1.0394811630249023
- 1.025023102760315
- 0.9754571318626404
- 0.9588940739631653
- 1.0280582904815674
- 1.0606499910354614
- 0.7847198247909546
- 0.7348899245262146
- 0.8456677198410034
- 0.9727696776390076
- 0.8480901122093201
- 0.894025444984436
- 0.6614829301834106
- 0.7995514869689941
- 0.6575934290885925
- 0.6224209070205688
- 0.5873891115188599
- 0.5155811905860901
- 0.510481595993042
- 0.6742843389511108
- 0.7575218081474304
- 0.7440481185913086
- 0.713887095451355
- 0.6166369915008545
- 0.6929892301559448
- 0.6903130412101746
- 0.5264562368392944
- 0.575796902179718
- 0.5452700257301331
- 0.4592585861682892
- 0.5063530206680298
- 0.39527326822280884
- 0.3275419771671295
- 0.4847989082336426
- 0.4419400691986084
- 0.4043644368648529
- 0.5499468445777893
- 0.35411202907562256
- 0.2780977487564087
- 0.15515126287937164
- -0.0037564323283731937
- 0.02674386091530323
- 0.03410471975803375
- 0.0058423294685781
- -0.015647990629076958
- 0.0037763931322842836
- -0.010835481807589531
- -0.23265041410923004
- -0.40023574233055115
- -0.45281127095222473
- -0.4746738374233246
- -0.39277946949005127
- -0.3967736065387726
- -0.3774961531162262
- -0.6243470311164856
- -0.6237818002700806
- -0.5036984086036682
- -0.5443686246871948
- -0.6719537973403931
- -0.6208534240722656
- -0.49432307481765747
- -0.9626864790916443
- -1.0958350896835327
- -0.986503005027771
- -1.0925105810165405
- -1.0092830657958984
- -1.015738844871521
- -0.8953530788421631
- -0.7664831876754761
- -1.0003812313079834
- -1.0096733570098877
- -0.9200026392936707
- -1.0280593633651733
- -1.0624552965164185
- -1.080880880355835
- -1.2438361644744873
- -1.1554944515228271
- -1.1604230403900146
- -1.405261516571045
- -1.2903525829315186
- -1.456544041633606
- -1.3246434926986694
- -1.3816852569580078
- -1.2928524017333984
- -1.2444469928741455
- -1.385374665260315
- -1.333848476409912
spec_min:
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
spk_cond_steps: []
stop_token_weight: 5.0
task_cls: training.task.SVC_task.SVCTask
test_ids: []
test_input_dir: ''
test_num: 0
test_prefixes:
- test
test_set_name: test
timesteps: 1000
train_set_name: train
use_crepe: false
use_denoise: false
use_energy_embed: false
use_gt_dur: false
use_gt_f0: false
use_midi: false
use_nsf: true
use_pitch_embed: true
use_pos_embed: true
use_spk_embed: false
use_spk_id: false
use_split_spk_id: false
use_uv: false
use_var_enc: false
use_vec: false
val_check_interval: 2000
valid_num: 0
valid_set_name: valid
vocoder: network.vocoders.nsf_hifigan.NsfHifiGAN
vocoder_ckpt: checkpoints/nsf_hifigan/model
warmup_updates: 2000
wav2spec_eps: 1e-6
weight_decay: 0
win_size: 2048
work_dir: /content/drive/MyDrive/diff-svc/BMO
