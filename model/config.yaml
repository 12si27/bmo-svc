K_step: 1000
accumulate_grad_batches: 1
audio_num_mel_bins: 128
audio_sample_rate: 44100
binarization_args:
  shuffle: false
  with_align: true
  with_f0: true
  with_hubert: true
  with_spk_embed: false
  with_wav: false
binarizer_cls: preprocessing.SVCpre.SVCBinarizer
binary_data_dir: data/binary/BMO
check_val_every_n_epoch: 10
choose_test_manually: false
clip_grad_norm: 1
config_path: training/config_nsf.yaml
content_cond_steps: []
credits: This model has been trained using liee (feminine). Please remember to credit
  the author of the model during usage.
cwt_add_f0_loss: false
cwt_hidden_size: 128
cwt_layers: 2
cwt_loss: l1
cwt_std_scale: 0.8
datasets:
- opencpop
debug: false
dec_ffn_kernel_size: 9
dec_layers: 4
decay_steps: 40000
decoder_type: fft
dict_dir: ''
diff_decoder_type: wavenet
diff_loss_type: l2
dilation_cycle_length: 4
dropout: 0.1
ds_workers: 4
dur_enc_hidden_stride_kernel:
- 0,2,3
- 0,2,3
- 0,1,3
dur_loss: mse
dur_predictor_kernel: 3
dur_predictor_layers: 5
enc_ffn_kernel_size: 9
enc_layers: 4
encoder_K: 8
encoder_type: fft
endless_ds: true
f0_bin: 256
f0_max: 1100.0
f0_min: 40.0
ffn_act: gelu
ffn_padding: SAME
fft_size: 2048
fmax: 16000
fmin: 40
fs2_ckpt: ''
gaussian_start: true
gen_dir_name: ''
gen_tgt_spk_id: -1
hidden_size: 256
hop_size: 512
hubert_gpu: true
hubert_path: checkpoints/hubert/hubert_soft.pt
infer: false
keep_bins: 128
lambda_commit: 0.25
lambda_energy: 0.0
lambda_f0: 1.0
lambda_ph_dur: 0.3
lambda_sent_dur: 1.0
lambda_uv: 1.0
lambda_word_dur: 1.0
load_ckpt: pretrain/model_ckpt_steps_600000.ckpt
log_interval: 100
loud_norm: false
lr: 0.0008
max_beta: 0.02
max_epochs: 3000
max_eval_sentences: 1
max_eval_tokens: 60000
max_frames: 42000
max_input_tokens: 60000
max_sentences: 12
max_tokens: 128000
max_updates: 1000000
mel_loss: ssim:0.5|l1:0.5
mel_vmax: 1.5
mel_vmin: -6.0
min_level_db: -120
no_fs2: true
norm_type: gn
num_ckpt_keep: 10
num_heads: 2
num_sanity_val_steps: 1
num_spk: 1
num_test_samples: 0
num_valid_plots: 10
optimizer_adam_beta1: 0.9
optimizer_adam_beta2: 0.98
out_wav_norm: false
pe_ckpt: checkpoints/0102_xiaoma_pe/model_ckpt_steps_60000.ckpt
pe_enable: false
perform_enhance: true
pitch_ar: false
pitch_enc_hidden_stride_kernel:
- 0,2,5
- 0,2,5
- 0,2,5
pitch_extractor: parselmouth
pitch_loss: l2
pitch_norm: log
pitch_type: frame
pndm_speedup: 10
pre_align_args:
  allow_no_txt: false
  denoise: false
  forced_align: mfa
  txt_processor: zh_g2pM
  use_sox: true
  use_tone: false
pre_align_cls: data_gen.singing.pre_align.SingingPreAlign
predictor_dropout: 0.5
predictor_grad: 0.1
predictor_hidden: -1
predictor_kernel: 5
predictor_layers: 5
prenet_dropout: 0.5
prenet_hidden_size: 256
pretrain_fs_ckpt: ''
processed_data_dir: xxx
profile_infer: false
raw_data_dir: data/raw/BMO
ref_norm_layer: bn
rel_pos: true
reset_phone_dict: true
residual_channels: 384
residual_layers: 20
save_best: false
save_ckpt: true
save_codes:
- configs
- modules
- src
- utils
save_f0: true
save_gt: false
schedule_type: linear
seed: 1234
sort_by_len: true
speaker_id: BMO
spec_max:
- 0.3086455762386322
- -0.05081344023346901
- -0.19828401505947113
- -0.0834435299038887
- 0.32919377088546753
- 0.7401334643363953
- 0.8476316928863525
- 0.9772645831108093
- 0.9442583918571472
- 1.1599498987197876
- 1.106584072113037
- 1.0887508392333984
- 1.1090424060821533
- 1.0557878017425537
- 1.064404010772705
- 1.1336365938186646
- 1.124609351158142
- 1.0444201231002808
- 1.0729221105575562
- 1.0794498920440674
- 1.1317384243011475
- 1.1482346057891846
- 1.0506930351257324
- 1.0020092725753784
- 1.0433634519577026
- 1.1376157999038696
- 1.1245145797729492
- 1.1065272092819214
- 1.1475305557250977
- 1.1218607425689697
- 1.1387776136398315
- 1.1683497428894043
- 1.1163915395736694
- 1.0732513666152954
- 1.1956744194030762
- 1.1639430522918701
- 1.0893000364303589
- 1.1351505517959595
- 1.0529818534851074
- 0.986912190914154
- 1.0486993789672852
- 1.0394811630249023
- 1.025023102760315
- 0.9754571318626404
- 0.9588940739631653
- 1.0280582904815674
- 1.0606499910354614
- 0.7847198247909546
- 0.7053459882736206
- 0.8456677198410034
- 0.9727696776390076
- 0.8480901122093201
- 0.894025444984436
- 0.6803500056266785
- 0.7995514869689941
- 0.6575934290885925
- 0.6502499580383301
- 0.6410873532295227
- 0.5155811905860901
- 0.6076975464820862
- 0.7683791518211365
- 0.7575218081474304
- 0.7440481185913086
- 0.7138870358467102
- 0.6166369915008545
- 0.6929892301559448
- 0.6903130412101746
- 0.5264562368392944
- 0.6001316905021667
- 0.7345625162124634
- 0.4592585861682892
- 0.5063530206680298
- 0.39527326822280884
- 0.5432136058807373
- 0.4847989082336426
- 0.39180025458335876
- 0.4043644368648529
- 0.5499469041824341
- 0.35411202907562256
- 0.2780977487564087
- 0.15515123307704926
- -0.0037564323283731937
- 0.02674386091530323
- 0.03410471975803375
- 0.005842380225658417
- -0.01564796268939972
- 0.0037763931322842836
- -0.1131492480635643
- -0.3198429346084595
- -0.40023574233055115
- -0.45281127095222473
- -0.4746738374233246
- -0.39277946949005127
- -0.3967735767364502
- -0.6845393776893616
- -0.6243470311164856
- -0.6237818002700806
- -0.5036984086036682
- -0.5443686842918396
- -0.6750397682189941
- -0.705237090587616
- -1.07133948802948
- -1.1274737119674683
- -1.1144163608551025
- -0.9691365361213684
- -0.8449488282203674
- -1.0839608907699585
- -0.8073625564575195
- -0.8856902718544006
- -0.7664831876754761
- -1.0003812313079834
- -1.0096733570098877
- -0.9200026392936707
- -1.0280592441558838
- -1.062455415725708
- -1.0808809995651245
- -1.1862645149230957
- -1.4025717973709106
- -1.238681435585022
- -1.3421623706817627
- -1.567363977432251
- -1.7948411703109741
- -1.691383957862854
- -1.9279484748840332
- -1.8723996877670288
- -1.8685592412948608
- -1.6333608627319336
- -1.7528010606765747
spec_min:
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.970791339874268
- -4.9588303565979
- -4.92976713180542
- -4.948894023895264
- -4.953942775726318
- -4.9563751220703125
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.999994277954102
- -4.983030319213867
- -4.84995698928833
- -4.710039138793945
- -4.634320259094238
- -4.515995025634766
- -4.417449951171875
spk_cond_steps: []
stop_token_weight: 5.0
task_cls: training.task.SVC_task.SVCTask
test_ids: []
test_input_dir: ''
test_num: 0
test_prefixes:
- test
test_set_name: test
timesteps: 1000
train_set_name: train
use_crepe: false
use_denoise: false
use_energy_embed: false
use_gt_dur: false
use_gt_f0: false
use_midi: false
use_nsf: true
use_pitch_embed: true
use_pos_embed: true
use_spk_embed: false
use_spk_id: false
use_split_spk_id: false
use_uv: false
use_var_enc: false
use_vec: false
val_check_interval: 2000
valid_num: 0
valid_set_name: valid
vocoder: network.vocoders.nsf_hifigan.NsfHifiGAN
vocoder_ckpt: checkpoints/nsf_hifigan/model
warmup_updates: 2000
wav2spec_eps: 1e-6
weight_decay: 0
win_size: 2048
work_dir: /content/drive/MyDrive/diff-svc/BMO
